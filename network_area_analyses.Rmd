---
title: "MS Network Area AF"
author: "Carine Emer"
date: "12/12/2018"
output: html_document
---

updating code from "metanetwork analyses March 2017.Rmd"(setwd("~/Dropbox/Seed dispersal data Atlantic/updated names/quanti")


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r set working directory}
setwd("~/Dropbox/MS network area AF/Analyses/network_area")
```


```{r packages}
library(bipartite); library(vegan); library(MuMIn)
library(lme4);library(nlme); library(fossil)
library(bipartiteD3); library(reshape2); library(reshape)

library(r2d3)
library(bipartite)
library(purrr) 
library(dplyr) 
library(tidyr) 
library(stringr)
library(tibble)
library(RColorBrewer)

library(bipartiteD3)
library(ggplot2)
library("cowplot")
library(igraph)

#load("network_area.RData")

```

```{r input data, include=FALSE}
setwd("~/Dropbox/MS network area AF/Analyses/network_area/data")
restoredPira<-read.csv("vivian quanti.csv",head=T,row.names=1)
fragmentRC<-read.csv("w34 Athie RC.csv",head=T,row.names=1) 
Cardoso<-read.csv("w44 Castro Cardoso.csv",head=T,row.names=1) 
RebioPcAntas<-read.csv("w30 Correia PcAntas.csv",head=T,row.names=1) 
fragmentMG<-read.csv("w36 Fadini MG.csv",head=T,row.names=1) 
restored15<-read.csv("fe15 quanti.csv",head=T,row.names=1) 
restored25<-read.csv("fe25 quanti.csv",head=T,row.names=1) 
restored57<-read.csv("fe57 quanti.csv",head=T,row.names=1) 
StaGenebra<-read.csv("w35 Galetti Pizo.csv",head=T,row.names=1) 
fragmentSP<-read.csv("w32 Hasui.csv",head=T,row.names=1) 
IAnchieta<-read.csv("w28 Kaiser original.csv",head=T,row.names=1) 
FAraucaria<-read.csv("w33 Kindel.csv",head=T,row.names=1) 
Itatiba<-read.csv("pizo quanti.csv",head=T,row.names=1) 
Intervales<-read.csv("w37 Wesley.csv",head=T,row.names=1)
CBotelho<-read.csv("w95 Botelho.csv",head=T,row.names=1)
fragmentRJ<-read.csv("w93 Rafael UFRJ.csv",head=T,row.names=1)

data<-read.csv("final_results_communities.csv", header=T) # original = CArdoso and Anchieta without added data on E. edulis, Clusia and Cryptocharia, removed all Psittacidae

```

```{r edgelist}
### can use 'melt' but has to be matrices
str(comm1)
comm1<-as.matrix(comm)


#####function to merge more than 2 dataframes, merge only does 2
MyMerge <- function(x, y){
  df <- merge(x, y, by= "cod", all.x= TRUE, all.y= T) ### change by = "cod" for interactions, Var1 for plants, Var2 - birds
  return(df)
}


###############################################
###### code for combine multiple matrices in a single one according to species and/or interactions

setwd("~/Dropbox/MS network area AF/Analyses/network_area/communities")
#getwd()
###################
###Species level
### getting the nested format for each matrix
v<-read.csv("n16 vivian quanti.csv", h=T, row.names = 1) 
v<-as.matrix(v)
df_v<-as.data.frame(as.table(v))
df_v$cod<-paste(df_v$Var1,df_v$Var2,sep = "_")
#write.csv(df_v$cod,"nested vivian.csv")
vivian_final<-df_v[,c(4,3)] #### ifelse p 0 and 1
vi<-vivian_final[!rowSums(vivian_final[-1] ==0),]

vi_plants<-df_v[,c(1,3)]
vi_p<-vi_plants[!rowSums(vi_plants[-1] ==0),]
vi_p<-vi_p[!duplicated(vi_p),]

vi_a<-df_v[,c(2,3)]
vi_a<-vi_a[!rowSums(vi_a[-1] ==0),]
vi_a[!duplicated(vi_a),]
vi_a<-unique(vi_a)

athie<-read.csv("n9 athie RC.csv", h=T, row.names = 1) 
athie<-as.matrix(athie)
df_v<-as.data.frame(as.table(athie))
df_v$cod<-paste(df_v$Var1,df_v$Var2,sep = "_")
#write.csv(df_v$cod1,"nested athie.csv")
athie_final<-df_v[,c(4,3)] #### 
at<-athie_final[!rowSums(athie_final[-1] ==0),]
head(at)
at_plants<-df_v[,c(1,3)]
at_p<-at_plants[!rowSums(at_plants[-1] ==0),]
at_p<-unique(at_p)
at_a<-df_v[,c(2,3)]
at_a<-at_a[!rowSums(at_a[-1] ==0),]
at_a<-unique(at_a)

castro<-read.csv("n3 Castro Cardoso.csv", h=T, row.names = 1) 
castro<-as.matrix(castro)
df_v<-as.data.frame(as.table(castro))
df_v$cod<-paste(df_v$Var1,df_v$Var2,sep = "_")
head(df_v$cod)
#write.csv(df_v$cod,"nested castro.csv")
castro_final<-df_v[,c(4,3)] #### 
cas<-castro_final[!rowSums(castro_final[-1] ==0),]

cas_plants<-df_v[,c(1,3)]
cas_p<-cas_plants[!rowSums(cas_plants[-1] ==0),]
cas_p<-unique(cas_p)
cas_a<-df_v[,c(2,3)]
cas_a<-cas_a[!rowSums(cas_a[-1] ==0),]
cas_a<-unique(cas_a)

correia<-read.csv("n4 correia PcAntas.csv", h=T,row.names = 1) 
v<-as.matrix(correia)
df_v<-as.data.frame(as.table(v))
df_v$cod<-paste(df_v$Var1,df_v$Var2,sep = "_")
#write.csv(df_v$cod,"nested correia.csv")
correia_final<-df_v[,c(4,3)] #### 
cor<-correia_final[!rowSums(correia_final[-1] ==0),]

cor_plants<-df_v[,c(1,3)]
cor_p<-cor_plants[!rowSums(cor_plants[-1] ==0),]
cor_p<-unique(cor_p)
cor_a<-df_v[,c(2,3)]
cor_a<-cor_a[!rowSums(cor_a[-1] ==0),]
cor_a<-unique(cor_a)

fadini<-read.csv("n10 Fadini MG.csv", h=T,row.names = 1) 
fadini<-as.matrix(fadini)
df_v<-as.data.frame(as.table(fadini))
df_v$cod<-paste(df_v$Var1,df_v$Var2,sep = "_")
#write.csv(df_v$cod,"nested fadini.csv")
fadini_final<-df_v[,c(4,3)] 
fad<-fadini_final[!rowSums(fadini_final[-1] ==0),]
fad_plants<-df_v[,c(1,3)]
fad_p<-fad_plants[!rowSums(fad_plants[-1] ==0),]
fad_p<-unique(fad_p)
fad_a<-df_v[,c(2,3)]
fad_a<-fad_a[!rowSums(fad_a[-1] ==0),]
fad_a<-unique(fad_a)


fe15<-read.csv("n13 fe15 quanti.csv", h=T, row.names = 1) 
fe15<-as.matrix(fe15)
df_v<-as.data.frame(as.table(fe15))
df_v$cod<-paste(df_v$Var1,df_v$Var2,sep = "_")
#write.csv(df_v$cod,"nested fe15.csv")
fe15_final<-df_v[,c(4,3)] ####
fe15<-fe15_final[!rowSums(fe15_final[-1] ==0),]
fe15_plants<-df_v[,c(1,3)]
fe15_p<-fe15_plants[!rowSums(fe15_plants[-1] ==0),]
fe15_p<-unique(fe15_p)
fe15_a<-df_v[,c(2,3)]
fe15_a<-fe15_a[!rowSums(fe15_a[-1] ==0),]
fe15_a<-unique(fe15_a)


fe25<-read.csv("n14 fe25 quanti.csv", h=T, row.names = 1) 
fe25<-as.matrix(fe25)
df_v<-as.data.frame(as.table(fe25))
df_v$cod<-paste(df_v$Var1,df_v$Var2,sep = "_")
#write.csv(df_v$cod,"nested fe25.csv")
fe25_final<-df_v[,c(4,3)] 
fe25<-fe25_final[!rowSums(fe25_final[-1] ==0),]
fe25_plants<-df_v[,c(1,3)]
fe25_p<-fe25_plants[!rowSums(fe25_plants[-1] ==0),]
fe25_p<-unique(fe25_p)
fe25_a<-df_v[,c(2,3)]
fe25_a<-fe25_a[!rowSums(fe25_a[-1] ==0),]
fe25_a<-unique(fe25_a)


fe57<-read.csv("n15 fe57 quanti.csv", h=T, row.names = 1) 
fe57<-as.matrix(fe57)
df_v<-as.data.frame(as.table(fe57))
df_v$cod<-paste(df_v$Var1,df_v$Var2,sep = "_")
#write.csv(df_v$cod,"nested fe57.csv")
fe57_final<-df_v[,c(4,3)]
fe57<-fe57_final[!rowSums(fe57_final[-1] ==0),]
fe57_plants<-df_v[,c(1,3)]
fe57_p<-fe57_plants[!rowSums(fe57_plants[-1] ==0),]
fe57_p<-unique(fe57_p)
fe57_a<-df_v[,c(2,3)]
fe57_a<-fe57_a[!rowSums(fe57_a[-1] ==0),]
fe57_a<-unique(fe57_a)

GP<-read.csv("n6 Galetti Pizo.csv", h=T, row.names = 1) 
gp<-as.matrix(GP)
df_v<-as.data.frame(as.table(gp))
df_v$cod<-paste(df_v$Var1,df_v$Var2,sep = "_")
#write.csv(df_v$cod,"nested Galetti Pizo.csv")
galetti_pizo_final<-df_v[,c(4,3)]
gp<-galetti_pizo_final[!rowSums(galetti_pizo_final[-1] ==0),]
gp_plants<-df_v[,c(1,3)]
gp_p<-gp_plants[!rowSums(gp_plants[-1] ==0),]
gp_p<-unique(gp_p)
gp_a<-df_v[,c(2,3)]
gp_a<-gp_a[!rowSums(gp_a[-1] ==0),]
gp_a<-unique(gp_a)

hasui<-read.csv("n11 Hasui.csv", h=T, row.names = 1) 
hasui<-as.matrix(hasui)
df_v<-as.data.frame(as.table(hasui))
df_v$cod<-paste(df_v$Var1,df_v$Var2,sep = "_")
#write.csv(df_v$cod,"nested Hasui.csv")
hasui_final<-df_v[,c(4,3)]
ha<-hasui_final[!rowSums(hasui_final[-1] ==0),]
ha_plants<-df_v[,c(1,3)]
ha_p<-ha_plants[!rowSums(ha_plants[-1] ==0),]
ha_p<-unique(ha_p)
ha_a<-df_v[,c(2,3)]
ha_a<-ha_a[!rowSums(ha_a[-1] ==0),]
ha_a<-unique(ha_a)

kaizer<-read.csv("n5 Kaiser original.csv", h=T, row.names = 1) 
kaizer<-as.matrix(kaizer)
df_v<-as.data.frame(as.table(kaizer))
df_v$cod<-paste(df_v$Var1,df_v$Var2,sep = "_")
#write.csv(df_v$cod,"nested kaiser.csv")
kaizer_final<-df_v[,c(4,3)]
ka<-kaizer_final[!rowSums(kaizer_final[-1] ==0),]
ka_plants<-df_v[,c(1,3)]
ka_p<-ka_plants[!rowSums(ka_plants[-1] ==0),]
ka_p<-unique(ka_p)
ka_a<-df_v[,c(2,3)]
ka_a<-ka_a[!rowSums(ka_a[-1] ==0),]
ka_a<-unique(ka_a)


kindel<-read.csv("n8 Kindel.csv", h=T, row.names = 1) 
kindel<-as.matrix(kindel)
df_v<-as.data.frame(as.table(kindel))
df_v$cod<-paste(df_v$Var1,df_v$Var2,sep = "_")
write.csv(df_v$cod,"nested kindel.csv")
kindel_final<-df_v[,c(4,3)]
ki<-kindel_final[!rowSums(kindel_final[-1] ==0),]
ki_plants<-df_v[,c(1,3)]
ki_p<-ki_plants[!rowSums(ki_plants[-1] ==0),]
ki_p<-unique(ki_p)
ki_a<-df_v[,c(2,3)]
ki_a<-ki_a[!rowSums(ki_a[-1] ==0),]
ki_a<-unique(ki_a)


pizo<-read.csv("n7 pizo quanti.csv", h=T, row.names = 1) 
pizo<-as.matrix(pizo)
df_v<-as.data.frame(as.table(pizo))
df_v$cod<-paste(df_v$Var1,df_v$Var2,sep = "_")
#write.csv(df_v$cod,"nested pizo.csv")
pizo_final<-df_v[,c(4,3)]
pi<-pizo_final[!rowSums(pizo_final[-1] ==0),]
pi_plants<-df_v[,c(1,3)]
pi_p<-pi_plants[!rowSums(pi_plants[-1] ==0),]
pi_p<-unique(pi_p)
pi_a<-df_v[,c(2,3)]
pi_a<-pi_a[!rowSums(pi_a[-1] ==0),]
pi_a<-unique(pi_a)

silva<-read.csv("n1 Wesley.csv", h=T, row.names = 1)# 
silva<-as.matrix(silva)
df_v<-as.data.frame(as.table(silva))
df_v$cod<-paste(df_v$Var1,df_v$Var2,sep = "_")
#write.csv(df_v$cod,"nested silva.csv")
silva_final<-df_v[,c(4,3)]
si<-silva_final[!rowSums(silva_final[-1] ==0),]
si_plants<-df_v[,c(1,3)]
si_p<-si_plants[!rowSums(si_plants[-1] ==0),]
si_p<-unique(si_p)
si_a<-df_v[,c(2,3)]
si_a<-si_a[!rowSums(si_a[-1] ==0),]
si_a<-unique(si_a)


botelho<-read.csv("n2 Botelho.csv", h=T,row.names = 1)# 
bo<-as.matrix(botelho)
df_v<-as.data.frame(as.table(bo))
df_v$cod<-paste(df_v$Var1,df_v$Var2,sep = "_")
#write.csv(df_v$cod,"nested botelho.csv")
botelho_final<-df_v[,c(4,3)]
bo<-botelho_final[!rowSums(botelho_final[-1] ==0),]
bo_plants<-df_v[,c(1,3)]
bo_p<-bo_plants[!rowSums(bo_plants[-1] ==0),]
bo_p<-unique(bo_p)
bo_a<-df_v[,c(2,3)]
bo_a<-bo_a[!rowSums(bo_a[-1] ==0),]
bo_a<-unique(bo_a)

rafael<-read.csv("n12 Rafael UFRJ.csv", h=T, row.names = 1)# 
ra<-as.matrix(rafael)
df_v<-as.data.frame(as.table(ra))
df_v$cod<-paste(df_v$Var1,df_v$Var2,sep = "_") # paste the combination in the new column
#write.csv(df_v$cod,"nested rafael.csv")
rafael_final<-df_v[,c(4,3)] ## extrai apenas colunas 4 e 3 para novo df
ra<-rafael_final[!rowSums(rafael_final[-1] ==0),] ### extrai somente freq >0 
ra_plants<-df_v[,c(1,3)]
ra_p<-ra_plants[!rowSums(ra_plants[-1] ==0),]
ra_p<-unique(ra_p)
ra_a<-df_v[,c(2,3)]
ra_a<-ra_a[!rowSums(ra_a[-1] ==0),]
ra_a<-unique(ra_a)


#########
#####function to merge more than 2 dataframes, merge only does 2
MyMerge <- function(x, y){
  df <- merge(x, y, by= "cod", all.x= TRUE, all.y= T) ### change by = "cod" for interactions, Var1 for plants, Var2 - birds
  return(df)
} 
##### if you wanna keep just the common variables (excluding NA, use: all.x= FALSE, all.y= FALSE

nested<- Reduce(MyMerge, list(si, bo,cas, cor, ka, fad, ki, gp,pi, fe25,at,fe15,            fe57,ra,ha,vi))


n1<-nested[,-1]
n2<-nested[,1]
#n1[n1>0]<-1 ### to replace all values >0 for 1
nested[is.na(nested)]<-0 ## to replace NA for zeros
n3<-cbind(n2,n1) # combine columns again
n3[is.na(n3)]<-0
head(n3)
t1 <- as.matrix(t(n3))

write.csv(t1, "nested_interactions.csv")
### nested for plants - remember changing the function "by"
nested_p<- Reduce(MyMerge, list(si_p, bo_p,cas_p, cor_p,ka_p,fad_p,ki_p,gp_p,pi_p,fe25_p,
                                at_p, fe15_p,fe57_p,ra_p,ha_p,vi_p)) # ordened by area

### nested for plants - remember changing the function "by"
nested_i<- Reduce(MyMerge, list(si, bo,cas,cor,ka,fad,ki,gp,pi,fe25,
                                at, fe15,fe57,ra,ha,vi)) # ordened by area



nested_i[is.na(nested_i)]<-0
t1 <- as.matrix(t(nested_i))

rownames(t1)[1]<-" "
rownames(t1)[2]<-"Intervales"
rownames(t1)[3]<-"CBotelho"
rownames(t1)[4]<-"Cardoso"
rownames(t1)[5]<-"RebioPcAntas"
rownames(t1)[6]<-"IAnchieta"
rownames(t1)[7]<-"fragmentMG"
rownames(t1)[8]<-"AraucariaF"
rownames(t1)[9]<-"StaGenebra"
rownames(t1)[10]<-"Itatiba"
rownames(t1)[11]<-"restored25"
rownames(t1)[12]<-"fragmentRC"
rownames(t1)[13]<-"restored15"
rownames(t1)[14]<-"restored57"
rownames(t1)[15]<-"fragmentRJ"
rownames(t1)[16]<-"fragmentSP"
rownames(t1)[17]<-"restored8"


################# distribution frequency
events<-read.csv("distribution.csv", header = T, row.names = 1)
events<-distribution
hist(log(events$N), xlab = "Number of seed-dispersal events (log)", main = "")


```

```{r modularity}
comm<-list(Intervales,CBotelho,Cardoso,RebioPcAntas,IAnchieta,fragmentMG,
           FAraucaria,StaGenebra,Itatiba,fragmentRC,restored57,
           restored15,restored25,fragmentSP,restoredPira,fragmentRJ)
names_comm<- c("Intervales","CBotelho","Cardoso","RebioPcAntas","IAnchieta","fragmentMG",
           "FAraucaria","StaGenebra","Itatiba","fragmentRC","restored57",
           "restored15","restored25","fragmentSP","restoredPira","fragmentRJ")

names(comm) <- names_comm

mod<-sapply(comm,computeModules)
mod[[1]]
mod_comm<-c(mod[[1]]@likelihood,mod[[2]]@likelihood,mod[[3]]@likelihood,mod[[4]]@likelihood,mod[[5]]@likelihood,mod[[6]]@likelihood,mod[[7]]@likelihood,mod[[8]]@likelihood,mod[[9]]@likelihood,mod[[10]]@likelihood,mod[[11]]@likelihood,mod[[12]]@likelihood,mod[[13]]@likelihood,mod[[14]]@likelihood,mod[[15]]@likelihood,mod[[16]]@likelihood)

data<-cbind(data,mod)

#############

Msig <- function (x, y)  {
    require(bipartite)
    # mat is the input matrix for which M is tested
    # mlike is the observed mean M value
    nulls <- nullmodel(x, N=100, method=3) ## 2 = swapweb, connectance and marginal totals constant (ie, constrait degree for sites and interactions/spp)
    modules.nulls <- sapply(nulls, computeModules)
    like.nulls <- sapply(modules.nulls, function(x) x@likelihood)
    z <- (y - mean(like.nulls))/sd(like.nulls)
    p <- 2*pnorm(-abs(z))
}
z_mod1<-Msig(comm[[1]],mod[[1]]@likelihood)
z_mod2<-Msig(comm[[2]],mod[[2]]@likelihood)
z_mod3<-Msig(comm[[3]],mod[[3]]@likelihood)
z_mod4<-Msig(comm[[4]],mod[[4]]@likelihood)
z_mod5<-Msig(comm[[5]],mod[[5]]@likelihood)
z_mod6<-Msig(comm[[6]],mod[[6]]@likelihood)
z_mod7<-Msig(comm[[7]],mod[[7]]@likelihood)
z_mod8<-Msig(comm[[8]],mod[[8]]@likelihood)
z_mod9<-Msig(comm[[9]],mod[[9]]@likelihood)
z_mod10<-Msig(comm[[10]],mod[[10]]@likelihood)
z_mod11<-Msig(comm[[11]],mod[[11]]@likelihood)
z_mod12<-Msig(comm[[12]],mod[[12]]@likelihood)
z_mod13<-Msig(comm[[13]],mod[[13]]@likelihood)
z_mod14<-Msig(comm[[14]],mod[[14]]@likelihood)
z_mod15<-Msig(comm[[15]],mod[[15]]@likelihood)
z_mod16<-Msig(comm[[16]],mod[[16]]@likelihood)

z_mod<-c(z_mod1,z_mod2,z_mod3,z_mod4,z_mod5,z_mod6,z_mod7,z_mod8,z_mod9,z_mod10,z_mod11,z_mod12,z_mod13,z_mod14,z_mod15,z_mod16)
data<-cbind(data,z_mod)

plot(log(data$z_mod)~log(data$area_ha)) 

############## Spatial structure
## first, REML to compare fixed terms
m1<-gls(z_mod ~ intensity+logarea+forest.type, data=data)# 
#m2<-gls(wnodf ~ logsize+logarea+forest.type, data=data,method="REML")

m1a<-update(m1,correlation=corSpher(form=~x+y))
m1b<-update(m1,correlation=corLin(form=~x+y))
m1c<-update(m1,correlation=corRatio(form=~x+y))
m1d<-update(m1,correlation=corGaus(form=~x+y))
m1e<-update(m1,correlation=corExp(form=~x+y))

AICc(m1,m1a,m1b,m1c,m1d,m1e) ### m1b best model

### second, fit ML for best model selection Zuur, pg 90
m1<-gls(z_mod~ intensity+logarea+forest.type, data=data, method="ML")
summary(m1)
dd<-dredge(m1, fixed = "intensity")
print(dd)
summary(get.models(dd,1)[[1]])
summary(get.models(dd,2)[[1]])

## finally, refit best models with REML for final results
m1best<-gls(z_mod ~ intensity, data=data)
summary (m1best)

m2best<-gls(z_mod ~ intensity +logarea, data=data)
summary (m2best)




#####
#<- <- <- <- <- <- <- <- <- <- <- <-  model suggested Rev 2
  ##### fit model
  mod1 <- lm(mod_comm ~ logarea + forest.type + intensity,
             data = data, na.action = "na.fail")
  ##### create model set with sampling intensity kept fixed
  mset1 <- dredge(mod1, fixed = "intensity")
  ##### extract parameter names from model matrix
  pars1 <- colnames(model.matrix(mod1))
  ##### extract full model-averaged parameters
  coefM1 <- model.avg(mset1)$coefficients[1, pars1]
  ##### extract model-averaged confidence intervals
  ciM1 <- confint(model.avg(mset1))[pars1, ]
  ##### create table with model-averaged parameter estimates and CIs
  (mtab1 <- cbind(coefM1, ciM1))
  ##### extract variable importance
  importance(mset1)



############## Spatial structure

## first, REML to compare fixed terms
m1<-gls(z_mod ~ intensity+logarea+forest.type, data=data,method="REML")# 
#m2<-gls(wnodf ~ logsize+logarea+forest.type, data=data,method="REML")

m1a<-update(m1,correlation=corSpher(form=~x+y))
m1b<-update(m1,correlation=corLin(form=~x+y))
m1c<-update(m1,correlation=corRatio(form=~x+y))
m1d<-update(m1,correlation=corGaus(form=~x+y))
m1e<-update(m1,correlation=corExp(form=~x+y))

AICc(m1,m1a,m1b,m1c,m1d,m1e) ### m1 best model

### second, fit ML for best model selection Zuur, pg 90
m1<-gls(z_mod~ intensity+logarea+forest.type, data=data, method="ML")
summary(m1)
dd<-dredge(m1, fixed = "intensity")
print(dd)
summary(get.models(dd,1)[[1]])
summary(get.models(dd,2)[[1]])

## finally, refit best models with REML for final results
m1best<-m1<-gls(z_mod ~ intensity+logarea, data=data, method="REML")
summary (m1best)



```

```{r Nestedness null model}

########## WNodf
wnodf<-sapply(comm, networklevel, index = "weighted NODF")
cor.test(mod_comm,h2_comm) 
plot(wnodf,mod_comm)
cor(wnodf,h2_comm)

plot(data$evenness~logarea)
cor(data$evenness,data$h2)
cor(data$evenness,data$wnodf)
cor(data$evenness,data$dirt_mod_quanti)

############## Spatial structure
hist(data$z_wnodf)
## 
m1<-gls(z_wnodf_3 ~ intensity+logarea+forest.type, data=data)
m1a<-update(m1,correlation=corSpher(form=~x+y))
m1b<-update(m1,correlation=corLin(form=~x+y))
m1c<-update(m1,correlation=corRatio(form=~x+y))
m1d<-update(m1,correlation=corGaus(form=~x+y))
m1e<-update(m1,correlation=corExp(form=~x+y))

AICc(m1,m1a,m1b,m1c,m1d,m1e) ### m1 best model

### second, fit ML for best model selection Zuur, pg 90
m1<-gls(z_wnodf_3~ intensity+logarea+forest.type, data=data, method="ML")
summary(m1)
dd<-dredge(m1, fixed = "intensity")
print(dd)
summary(get.models(dd,1)[[1]])
summary(get.models(dd,2)[[1]])

## finally, refit best models with REML for final results
m1best<-lm(z_wnodf_1 ~ intensity, data=data)#, method="REML")
summary (m1best)

#-------------------------------- function null model
NODFsig <- function (x, y)  {
    require(bipartite)
    # x is the input matrix for which wNODF is tested
    # obs is the observed wNODF value
    obs<-networklevel(x, index = "weighted NODF")
    nulls <- nullmodel(x, N=100, method=3) 
    wnodf.nulls <- sapply(nulls, networklevel, index = "weighted NODF")
    z <- (obs - mean(wnodf.nulls))/sd(wnodf.nulls)
    #p <- 2*pnorm(-abs(z))
}    


z_wnodf1<-NODFsig(comm[[1]])
z_wnodf2<-NODFsig(comm[[2]])
z_wnodf3<-NODFsig(comm[[3]])
z_wnodf4<-NODFsig(comm[[4]])
z_wnodf5<-NODFsig(comm[[5]])
z_wnodf6<-NODFsig(comm[[6]])
z_wnodf7<-NODFsig(comm[[7]])
z_wnodf8<-NODFsig(comm[[8]])
z_wnodf9<-NODFsig(comm[[9]])
z_wnodf10<-NODFsig(comm[[10]])
z_wnodf11<-NODFsig(comm[[11]])
z_wnodf12<-NODFsig(comm[[12]])
z_wnodf13<-NODFsig(comm[[13]])
z_wnodf14<-NODFsig(comm[[14]])
z_wnodf15<-NODFsig(comm[[15]])
z_wnodf16<-NODFsig(comm[[16]])


z_wnodf_3<-c(z_wnodf1,z_wnodf2,z_wnodf3,z_wnodf4,z_wnodf5,z_wnodf6,z_wnodf7,z_wnodf8,z_wnodf9,z_wnodf10,z_wnodf11,z_wnodf12,z_wnodf13,z_wnodf14,z_wnodf15,z_wnodf16)
data<-cbind(data,z_wnodf_3)
plot((data$z_wnodf_3)~log(data$area_ha))
cor(z_wnodf_3,z_wnodf_1)

#####
#<- <- <- <- <- <- <- <- <- <- <- <-  model suggested Rev 2
  ##### fit model
  mod1 <- lm(wnodf ~ logarea + forest.type + intensity,
             data = data, na.action = "na.fail")
  ##### create model set with sampling intensity kept fixed
  mset1 <- dredge(mod1, fixed = "intensity")
  ##### extract parameter names from model matrix
  pars1 <- colnames(model.matrix(mod1))
  ##### extract full model-averaged parameters
  coefM1 <- model.avg(mset1)$coefficients[1, pars1]
  ##### extract model-averaged confidence intervals
  ciM1 <- confint(model.avg(mset1))[pars1, ]
  ##### create table with model-averaged parameter estimates and CIs
  (mtab1 <- cbind(coefM1, ciM1))
  ##### extract variable importance
  importance(mset1)
  
  

m1_avmod <- model.avg(mset1)
summary(m1_avmod)





```


```{r h2 null model}


#-------------------------------- function null model
H2sig <- function (x, y)  {
    require(bipartite)
    # x is the input matrix for which wNODF is tested
    # obs is the observed wNODF value
    obs<-networklevel(x, index = "H2")
    nulls <- nullmodel(x, N=100, method=1) 
    h2.nulls <- sapply(nulls, networklevel, index = "H2")
    z <- (obs - mean(h2.nulls))/sd(h2.nulls)
    #p <- 2*pnorm(-abs(z))
}    


z_h2_1<-H2sig(comm[[1]])
z_h2_2<-H2sig(comm[[2]])
z_h2_3<-H2sig(comm[[3]])
z_h2_4<-H2sig(comm[[4]])
z_h2_5<-H2sig(comm[[5]])
z_h2_6<-H2sig(comm[[6]])
z_h2_7<-H2sig(comm[[7]])
z_h2_8<-H2sig(comm[[8]])
z_h2_9<-H2sig(comm[[9]])
z_h2_10<-H2sig(comm[[10]])
z_h2_11<-H2sig(comm[[11]])
z_h2_12<-H2sig(comm[[12]])
z_h2_13<-H2sig(comm[[13]])
z_h2_14<-H2sig(comm[[14]])
z_h2_15<-H2sig(comm[[15]])
z_h2_16<-H2sig(comm[[16]])


z1_h2<-c(z_h2_1,z_h2_2,z_h2_3,z_h2_4,z_h2_5,z_h2_6,z_h2_7,z_h2_8,z_h2_9,z_h2_10,z_h2_11,z_h2_12,z_h2_13,z_h2_14,z_h2_15,z_h2_16)
data<-cbind(data,z1_h2)
plot((data$z_h2)~log(data$area_ha))
cor(z_h2,z1_h2)


########## H2
h2

############## Spatial structure
hist(data$z_h2)
## 
m1<-gls(z1_h2 ~ intensity+logarea+forest.type, data=data)
m1a<-update(m1,correlation=corSpher(form=~x+y))
m1b<-update(m1,correlation=corLin(form=~x+y))
m1c<-update(m1,correlation=corRatio(form=~x+y))
m1d<-update(m1,correlation=corGaus(form=~x+y))
m1e<-update(m1,correlation=corExp(form=~x+y))

AICc(m1,m1a,m1b,m1c,m1d,m1e) ### m1 best model

### second, fit ML for best model selection Zuur, pg 90
m1<-gls(z1_h2~ intensity+logarea+forest.type, data=data, method="ML")
summary(m1)
dd<-dredge(m1, fixed = "intensity")
print(dd)
summary(get.models(dd,1)[[1]])
summary(get.models(dd,2)[[1]])

## finally, refit best models with REML for final results
m1_z1_h2<-lm(z1_h2~ intensity, data=data)#, method="REML")
summary (m1_z1_h2)

```



```{r links per species}
#_____________________________________ method Icaro
#links per species
net.metrics.links <- lapply(comm, networklevel, index = 'links per species') 
net.metrics.links

# Make null models for all sites using the r2dtable null
net.nulls.r2d <- lapply(comm, nullmodel, method = "r2dtable", N = 100) 
#net.nulls.vaz <- lapply(comm, nullmodel, method = "vaznull", N = 100) 

# Null distribution function for links per species - calculates the network links per species metric for each null for each site (doesnt work for vaznull because it maintains connectance constant) 
net.null.links = function(nulls){
  net.null.metric <- list()
  for (i in 1:length(nulls)) {
    net.null.metric[[i]] = do.call('rbind', 
                                   lapply(nulls[[i]], networklevel, index = 'links per species'))
  }
  names(net.null.metric) <- names(comm)
  return(net.null.metric)
} 
r2d.links <- net.null.links(net.nulls.r2d)
#vaz.links <- net.null.links(net.nulls.vaz)
# Z-score function for comparing different networks
net.zscore = function(obsval, nullval) {
  (obsval - mean(nullval))/sd(nullval)
}

# Function that perform z-score calculation of links per species using the observed and null networks
links.zscore = function(nulltype){
  net.links.zscore <- list() 
  for(i in 1:length(net.metrics.links)){
    net.links.zscore[[i]] = net.zscore(net.metrics.links[[i]]['links per species'], 
                                       nulltype[[i]][ ,'links per species'])
  }
  names(net.links.zscore) <- names(comm)
  return(net.links.zscore)
}


r2d.links.zscore <- links.zscore(r2d.links)
#vaz.links.zscore <- links.zscore(vaz.links)
 # Change the output class from list of a list into a matrix
  net.metric.zscore <- do.call('rbind', r2d.links.zscore)
  #net.metric.zscore <- do.call('rbind', vaz.links.zscore)
 # Change matrix into a dataframe
  net.zlinks <- as.data.frame(as.table(net.metric.zscore))
  colnames(net.zlinks) <- c('site', 'links per sp', 'z_links')
  z_links<-net.zlinks$z_links

z1_links<-z_links    
  
data<-cbind(data, z_links)
data$z_links






# statistical models with z-score
############## Spatial structure
hist(data$z_links)
## 
m1<-gls(z_links ~ intensity+logarea+forest.type, data=data)
m1a<-update(m1,correlation=corSpher(form=~x+y))
m1b<-update(m1,correlation=corLin(form=~x+y))
m1c<-update(m1,correlation=corRatio(form=~x+y))
m1d<-update(m1,correlation=corGaus(form=~x+y))
m1e<-update(m1,correlation=corExp(form=~x+y))

AICc(m1,m1a,m1b,m1c,m1d,m1e) ### m1 best model

### second, fit ML for best model selection Zuur, pg 90
m1<-gls(z_links~ intensity+logarea+forest.type, data=data, method="ML")
summary(m1)
dd<-dredge(m1, fixed = "intensity")
print(dd)
summary(get.models(dd,1)[[1]])
summary(get.models(dd,2)[[1]])

## finally, refit best models with REML for final results
m1<-lm(z_links~ intensity, data=data)#, method="REML")
summary (m1)

```


```{r h2}

h2_comm<-sapply(comm,networklevel, index = "H2")
h2_comm
cor(h2_comm, links)

plot(log(data$area_ha), h2_comm)
plot(mod_comm, h2_comm, xlab = "Modularity", ylab = "H'2", main= "cor = 0.80")

H2sig <- function (x, y)  {
    require(bipartite)
    # x is the input matrix for which H2 is tested
    # obs is the observed H2 value
    obs<-networklevel(x, index = "H2")
    nulls <- nullmodel(x, N=100, method=3) 
    h2.nulls <- sapply(nulls, networklevel, index = "H2")
    z <- (obs - mean(h2.nulls))/sd(h2.nulls)
    #p <- 2*pnorm(-abs(z))
}    

z_h2_1<-H2sig(comm[[1]])
z_h2_2<-H2sig(comm[[2]])
z_h2_3<-H2sig(comm[[3]])
z_h2_4<-H2sig(comm[[4]])
z_h2_5<-H2sig(comm[[5]])
z_h2_6<-H2sig(comm[[6]])
z_h2_7<-H2sig(comm[[7]])
z_h2_8<-H2sig(comm[[8]])
z_h2_9<-H2sig(comm[[9]])
z_h2_10<-H2sig(comm[[10]])
z_h2_11<-H2sig(comm[[11]])
z_h2_12<-H2sig(comm[[12]])
z_h2_13<-H2sig(comm[[13]])
z_h2_14<-H2sig(comm[[14]])
z_h2_15<-H2sig(comm[[15]])
z_h2_16<-H2sig(comm[[16]])


z_h2<-c(z_h2_1,z_h2_2,z_h2_3,z_h2_4,z_h2_5,z_h2_6,z_h2_7,z_h2_8,z_h2_9,z_h2_10,z_h2_11,z_h2_12,z_h2_13,z_h2_14,z_h2_15,z_h2_16)
data<-cbind(data,z_h2)

plot((data$z_h2)~logarea)





```

```{r correlations}

plot(data$area)
logarea<-log(data$area)
plot(logarea) # better fit

cor(data$area,data$def2Jac) # -0.8
cor(data$area,data$def)
plot(data$def~logarea)

cor(logsize,logarea)
cor(logsize,data$c) # 0.47
cor(logsize,data$dirt_mod_quanti) ## only one not correlated
cor(logsize,data$wnodf)
cor(logsize,data$h2)

cor(logsize,data$intensity) #0.1
cor(logsize,loglinkdens) #0.9
cor(logsize,logarea) #0.69
cor(logsize,data$dirt_mod_quanti)
cor(logsize,data$c)
cor(logsize,data$wnodf)
cor(data$dirt_mod_quanti,(data$h2))

data1<-data[,c(14, 17, 21, 22,23,25)]
data2<-cbind(data1,logsize)
#data2<-cbind(data2,logarea)

pairs(data2, pch = 1)
data[13]

plot(log(data$size))
plot(logarea,data$c)
```


```{r sampling completeness}
str(comm)
chao<-sapply(comm, chao1)
chao
data<-cbind(data,chao)
data$chao


```

```{r models}
### model selection follows the protocol in Zuur et al. p 90, p.127, p.169


####logs= log size of the network and logarea = log(area)
plot((data$size))
logsize<-log(data$size) ## better fit
plot(logsize)

plot(data$area)
logarea<-log(data$area)
plot(logarea) # better fit

################animals##########################
#animals
hist(data$n.animals)
hist(log(data$n.animals))
animals<-log(data$n.animals)
## first, fit gls to test for spatial autocorrelation, Zuur p 169.
m1<-gls(animals ~ intensity+logarea+forest.type, data=data)# 
plot(m1)


m1a<-update(m1,correlation=corSpher(form=~x+y))
m1b<-update(m1,correlation=corLin(form=~x+y))
m1c<-update(m1,correlation=corRatio(form=~x+y))
m1d<-update(m1,correlation=corGaus(form=~x+y))
m1e<-update(m1,correlation=corExp(form=~x+y))

AICc(m1,m1a,m1b,m1c,m1d,m1e) ### m1 best model with lower AICc value

### second, fit ML for best model selection Zuur, pg 90
m1<-gls(animals~ intensity+logarea+forest.type, data=data, method="ML")
summary(m1)
dd<-dredge(m1, fixed = "intensity")
print(dd)
summary(get.models(dd,1)[[1]])

## finally, refit best models with REML for final results
m2best<-lm(animals~intensity+logarea, data=data)#, method="REML")
summary (m2best)

#library(msm)
#drop1(m2best, test= "Chisq")


  ##### model suggested Rev 2
  ##### fit model
  mod1 <- lm(animals ~ logarea + forest.type + intensity,
             data = data, na.action = "na.fail")
  ##### create model set with sampling intensity kept fixed
  mset1 <- dredge(mod1, fixed = "intensity")
  ##### extract parameter names from model matrix
  pars1 <- colnames(model.matrix(mod1))
  ##### extract full model-averaged parameters
  coefM1 <- model.avg(mset1)$coefficients[1, pars1]
  ##### extract model-averaged confidence intervals
  ciM1 <- confint(model.avg(mset1))[pars1, ]
  ##### create table with model-averaged parameter estimates and CIs
  (mtab1 <- cbind(coefM1, ciM1))
  ##### extract variable importance
  importance(mset1)

m1_avmod <- model.avg(mset1)
summary(m1_avmod)


###############plants##########################
#plants
hist(data$n.plants)
hist(log(data$n.plants))
plants<-log(data$n.plants)

m1<-gls(plants ~ intensity+logarea+forest.type, data=data)

m1a<-update(m1,correlation=corSpher(form=~x+y))
m1b<-update(m1,correlation=corLin(form=~x+y))
m1c<-update(m1,correlation=corRatio(form=~x+y))
m1d<-update(m1,correlation=corGaus(form=~x+y))
m1e<-update(m1,correlation=corExp(form=~x+y))

AICc(m1,m1a,m1b,m1c,m1d,m1e) ### m1 best model

### second, fit ML for best model selection Zuur, pg 90
m1<-gls(plants~ intensity+logarea+forest.type, data=data, method="ML")
summary(m1)
dd<-dredge(m1, fixed = "intensity")
print(dd)
summary(get.models(dd,1)[[1]])


## finally, refit best models with REML for final results
m1best<-lm(plants ~ logarea +intensity, data=data)
summary (m1best)

drop1(m1best, test= "Chisq")


 ##### <- <- <- <- <- <- <- <- <- <- <- <-  model suggested Rev 2
  ##### fit model
  mod1 <- lm(plants ~ logarea + forest.type + intensity,
             data = data, na.action = "na.fail")
  ##### create model set with sampling intensity kept fixed
  mset1 <- dredge(mod1, fixed = "intensity")
  ##### extract parameter names from model matrix
  pars1 <- colnames(model.matrix(mod1))
  ##### extract full model-averaged parameters
  coefM1 <- model.avg(mset1)$coefficients[1, pars1]
  ##### extract model-averaged confidence intervals
  ciM1 <- confint(model.avg(mset1))[pars1, ]
  ##### create table with model-averaged parameter estimates and CIs
  (mtab1 <- cbind(coefM1, ciM1))
  ##### extract variable importance
  importance(mset1)

m1_avmod <- model.avg(mset1)
summary(m1_avmod)


##############interactions###########################
hist(data$n.interactions)
hist(log(data$n.interactions))
interactions<-log(data$n.interactions)

## first, REML to compare fixed terms
m1<-gls(interactions ~ intensity+logarea+forest.type, data=data)# 


m1a<-update(m1,correlation=corSpher(form=~x+y))
m1b<-update(m1,correlation=corLin(form=~x+y))
m1c<-update(m1,correlation=corRatio(form=~x+y))
m1d<-update(m1,correlation=corGaus(form=~x+y))
m1e<-update(m1,correlation=corExp(form=~x+y))

AICc(m1,m1a,m1b,m1c,m1d,m1e) ### m1 best model

### second, fit ML for best model selection Zuur, pg 90
m1<-gls(interactions~ intensity+logarea+forest.type, data=data, method="ML")
summary(m1)
dd<-dredge(m1, fixed = "intensity")
print(dd)
summary(get.models(dd,1)[[1]])


## finally, refit best models with REML for final results
m1best<-lm(interactions ~ logarea, data=data)#, method="REML")
summary (m1best)

drop1(m1best, test = "Chisq")

##### <- <- <- <- <- <- <- <- <- <- <- <-  model suggested Rev 2
  ##### fit model
  mod1 <- lm(n.interactions ~ logarea + forest.type + intensity,
             data = data, na.action = "na.fail")
  ##### create model set with sampling intensity kept fixed
  mset1 <- dredge(mod1, fixed = "intensity")
  ##### extract parameter names from model matrix
  pars1 <- colnames(model.matrix(mod1))
  ##### extract full model-averaged parameters
  coefM1 <- model.avg(mset1)$coefficients[1, pars1]
  ##### extract model-averaged confidence intervals
  ciM1 <- confint(model.avg(mset1))[pars1, ]
  ##### create table with model-averaged parameter estimates and CIs
  (mtab1 <- cbind(coefM1, ciM1))
  ##### extract variable importance
  importance(mset1)

m1_avmod <- model.avg(mset1)
summary(m1_avmod)

##############connectance###########################
hist(data$c)
## first, REML to compare fixed terms
m1<-gls(log(c+0.0649)/(1-c+0.0649) ~ intensity+logarea+forest.type, data=data,method="REML")# 
#m1<-gls(log(c+0.0649)/(1-c+0.0649) ~ logsize+logarea+forest.type, data=data,method="REML")
#m3<-gls(c ~ logsize+res1+forest.type, data=data,method="REML")

#AIC(m1,m2)

m1a<-update(m1,correlation=corSpher(form=~x+y))
m1b<-update(m1,correlation=corLin(form=~x+y))
m1c<-update(m1,correlation=corRatio(form=~x+y))
m1d<-update(m1,correlation=corGaus(form=~x+y))
m1e<-update(m1,correlation=corExp(form=~x+y))

AIC(m1,m1a,m1b,m1c,m1d,m1e) ### m1 best model

### second, fit ML for best model selection Zuur, pg 90
m1<-gls(log(c+0.0649)/(1-c+0.0649)~ intensity+logarea+forest.type, data=data, method="ML")
summary(m1)
dd<-dredge(m1)
print(dd)
summary(get.models(dd,1)[[1]])
summary(get.models(dd,2)[[1]])

## finally, refit best models with REML for final results
m1best<-gls(log(c+0.0649)/(1-c+0.0649) ~ 1, data=data, method="REML")
summary (m1best)

m1best<-gls(log(c+0.0649)/(1-c+0.0649) ~ intensity, data=data, method="REML")
summary (m1best)

##############modularity###########################
hist(data$dirt_mod_quanti)
hist(z_mod)
## first, REML to compare fixed terms
m1<-gls(log(dirt_mod_quanti+0.2216441)/(1-dirt_mod_quanti+0.2216441) ~ intensity+logarea+forest.type, data=data)# 

m1a<-update(m1,correlation=corSpher(form=~x+y))
m1b<-update(m1,correlation=corLin(form=~x+y))
m1c<-update(m1,correlation=corRatio(form=~x+y))
m1d<-update(m1,correlation=corGaus(form=~x+y))
m1e<-update(m1,correlation=corExp(form=~x+y))

#### use AICc for small sample sizes
AICc(m1,m1a,m1c,m1d,m1e)


### second, fit ML for best model selection Zuur, pg 90
m1<-gls(log(dirt_mod_quanti+0.2216441)/(1-dirt_mod_quanti+0.2216441)~ intensity+logarea+forest.type, data=data, method="ML")
summary(m1)
dd<-dredge(m1, fixed = "intensity")
print(dd)
summary(get.models(dd,1)[[1]])
summary(get.models(dd,2)[[1]])

## finally, refit best models with REML for final results
m1best<-lm(log(dirt_mod_quanti+0.2216441)/(1-dirt_mod_quanti+0.2216441) ~ intensity, data=data)
summary (m1best)

##### ----------------- Modularity with z-scores
m1<-gls(z_mod ~ intensity+logarea+forest.type, data=data,method="REML")

### testing for spatial correlation
m1a<-update(m1,correlation=corSpher(form=~x+y))
m1b<-update(m1,correlation=corLin(form=~x+y))
m1c<-update(m1,correlation=corRatio(form=~x+y))
m1d<-update(m1,correlation=corGaus(form=~x+y))
m1e<-update(m1,correlation=corExp(form=~x+y))

AICc(m1,m1a,m1c,m1d,m1e)

### second, fit ML for best model selection Zuur, pg 90
m1<-gls(z_mod~ intensity+logarea+forest.type, data=data, method="ML")
summary(m1)
##### fit model
mod1 <- lm(z_mod~ intensity+logarea+forest.type, data=data, na.action = "na.fail")
##### create model set with sampling intensity kept fixed
dd<-dredge(mod1, fixed = "intensity")
print(dd)
summary(get.models(dd,1)[[1]])
summary(get.models(dd,2)[[1]])

#-----------------------------------------------------------------------------------
#### following Reviewer 2 Biotropica
##### fit model
mod1 <- lm(z_mod ~ intensity+logarea+forest.type,
           data = data, na.action = "na.fail")
##### create model set with sampling intensity kept fixed
mset1 <- dredge(mod1, fixed = "intensity")
##### extract parameter names from model matrix
pars1 <- colnames(model.matrix(mod1))
##### extract full model-averaged parameters
coefM1 <- model.avg(mset1)$coefficients[1, pars1]
##### extract model-averaged confidence intervals
ciM1 <- confint(model.avg(mset1))[pars1, ]
##### create table with model-averaged parameter estimates and CIs
(mtab1 <- cbind(coefM1, ciM1))
##### extract variable importance
importance(mset1)






##############H2###########################
hist(data$h2)

## first, REML to compare fixed terms
m1<-gls(log(h2+0.0673)/(1-h2+0.0673) ~ intensity+logarea+forest.type, data=data,method="REML")# 
#m1<-gls(log(h2+0.0673)/(1-h2+0.0673) ~ logsize+logarea+forest.type, data=data,method="REML")

m1a<-update(m1,correlation=corSpher(form=~x+y))
m1b<-update(m1,correlation=corLin(form=~x+y))
m1c<-update(m1,correlation=corRatio(form=~x+y))
m1d<-update(m1,correlation=corGaus(form=~x+y))
m1e<-update(m1,correlation=corExp(form=~x+y))

AIC(m1,m1a,m1b,m1c,m1d,m1e) ### m1 best model

### second, fit ML for best model selection Zuur, pg 90
m1<-gls(log(h2+0.0673)/(1-h2+0.0673)~ intensity+logarea+forest.type, data=data, method="ML")
summary(m1)
dd<-dredge(m1, fixed = "intensity")
print(dd)
summary(get.models(dd,1)[[1]])
summary(get.models(dd,2)[[1]])

## finally, refit best models with REML for final results
m1best<-lm(log(h2+0.0673)/(1-h2+0.0673) ~ intensity, data=data)
summary (m1best)
m2best<-lm(log(h2+0.0673)/(1-h2+0.0673) ~ logarea+intensity, data=data)
summary (m2best)


############## evenness ##########################

## first, REML to compare fixed term
m1<-gls(log(evenness+0.5514)/(1-evenness+0.5514) ~ intensity+logarea+forest.type, data=data)# 

m1a<-update(m1,correlation=corSpher(form=~x+y))
m1b<-update(m1,correlation=corLin(form=~x+y))
m1c<-update(m1,correlation=corRatio(form=~x+y))
m1d<-update(m1,correlation=corGaus(form=~x+y))
m1e<-update(m1,correlation=corExp(form=~x+y))

AICc(m1,m1a,m1b,m1c,m1d,m1e) ### m1 best model

### second, fit ML for best model selection Zuur, pg 90
m1<-gls(log(evenness+0.5514)/(1-evenness+0.5514)~ intensity+logarea+forest.type, data=data, method="ML")
summary(m1)
dd<-dredge(m1)#, fixed = "intensity")
print(dd)
summary(get.models(dd,1)[[1]])
summary(get.models(dd,2)[[1]])


## finally, refit best models with REML for final results
m1best<-lm(log(evenness+0.5514)/(1-evenness+0.5514) ~ intensity, data=data)
summary (m1best)

m2best<-lm(log(evenness+0.5514)/(1-evenness+0.5514) ~ logarea + intensity, data=data)
summary (m2best)

#####
#- <- <- <- <- <- <- <- <- <- <- <-  model suggested Rev 2
  ##### fit model
  mod1 <- lm(log(evenness+0.5514)/(1-evenness+0.5514) ~ logarea + forest.type + intensity,
             data = data, na.action = "na.fail")
  ##### create model set with sampling intensity kept fixed
  mset1 <- dredge(mod1, fixed = "intensity")
  ##### extract parameter names from model matrix
  pars1 <- colnames(model.matrix(mod1))
  ##### extract full model-averaged parameters
  coefM1 <- model.avg(mset1)$coefficients[1, pars1]
  ##### extract model-averaged confidence intervals
  ciM1 <- confint(model.avg(mset1))[pars1, ]
  ##### create table with model-averaged parameter estimates and CIs
  (mtab1 <- cbind(coefM1, ciM1))
  ##### extract variable importance
  importance(mset1)
  

m1_avmod <- model.avg(mset1)
summary(m1_avmod)







############# links per species ##########################
links
## first, REML to compare fixed term
m1<-gls(links ~ intensity+logarea+forest.type, data=data,method="REML")# 

m1a<-update(m1,correlation=corSpher(form=~x+y))
m1b<-update(m1,correlation=corLin(form=~x+y))
m1c<-update(m1,correlation=corRatio(form=~x+y))
m1d<-update(m1,correlation=corGaus(form=~x+y))
m1e<-update(m1,correlation=corExp(form=~x+y))

AIC(m1,m1a,m1b,m1c,m1d,m1e) ### m1 best model

### second, fit ML for best model selection Zuur, pg 90
m1<-gls(links~ intensity+logarea+forest.type, data=data, method="ML")
summary(m1)
dd<-dredge(m1, fixed = "intensity")
print(dd)
summary(get.models(dd,1)[[1]])

## finally, refit best models with REML for final results
m1best<-lm(links ~ logarea+intensity, data=data)
summary (m1best)



########## 
###NODF
## first, REML to compare fixed term
m1<-gls(wnodf ~ intensity+logarea+forest.type, data=data,method="REML")# 

m1a<-update(m1,correlation=corSpher(form=~x+y))
m1b<-update(m1,correlation=corLin(form=~x+y))
m1c<-update(m1,correlation=corRatio(form=~x+y))
m1d<-update(m1,correlation=corGaus(form=~x+y))
m1e<-update(m1,correlation=corExp(form=~x+y))

AIC(m1,m1a,m1b,m1c,m1d,m1e) ### m1 best model

### second, fit ML for best model selection Zuur, pg 90
m1<-gls(wnodf~ intensity+logarea+forest.type, data=data, method="ML")
summary(m1)
dd<-dredge(m1, fixed = "intensity")
print(dd)
summary(get.models(dd,1)[[1]])

## finally, refit best models with REML for final results
m1best<-lm(wnodf ~ logarea+intensity, data=data)
summary (m1best)

```

```{r ggplot, include = FALSE}

## ≠≠≠≠≠≠≠≠≠≠≠≠≠≠≠≠≠≠≠≠≠≠≠≠ function to set a legend for multiple panels
library(gridExtra)
get_legend<-function(myggplot){
  tmp <- ggplot_gtable(ggplot_build(myggplot))
  leg <- which(sapply(tmp$grobs, function(x) x$name) == "guide-box")
  legend <- tmp$grobs[[leg]]
  return(legend)
}
## ≠≠≠≠≠≠≠≠≠≠≠≠≠≠≠≠≠≠≠≠≠≠≠≠ 
# 1. Create the plots
#++++++++++++++++++++++++++++++++++
# 2. Save the legend
#+++++++++++++++++++++++
#legend <- get_legend(bp)
# 3. Remove the legend from the plot
#+++++++++++++++++++++++
#bp <- bp + theme(legend.position="none")
# 4. Arrange ggplot2 graphs with a specific width

### defaunation index used in Science paper
#def<-c(0.557,0.615,0.626,0.926,0.958,0.838,0.96,0.837,0.897,0.963,0.986,
  #     0.971,0.95,0.952,0.956,0.971)
#data<-cbind(data,def)
#1. Scatter plots
sp1 <- ggplot(data, aes(x = logarea, y = (n.animals), size = 4, fill=def)) +
             geom_point(shape = 21) +
             labs(x = "", y = "N bird species")
sp1<-sp1 + scale_fill_continuous(low = "#56B1F7", high = "#132B43") +
           theme(legend.position ="none")
sp1
## ≠≠≠≠≠≠≠≠≠≠≠≠≠≠≠≠≠≠≠≠≠≠≠≠
sp2 <- ggplot(data, aes(x = logarea, y = log(n.plants), size = 4, fill = def)) +
             geom_point(shape = 21) +
             labs(x = "", y = "N plant species (log)")
sp2<-sp2 + scale_fill_continuous(low = "#56B1F7", high = "#132B43") +
           theme(legend.position ="none")
sp2
## ≠≠≠≠≠≠≠≠≠≠≠≠≠≠≠≠≠≠≠≠≠≠≠≠
sp3 <- ggplot(data, aes(x = logarea, y = n.interactions, fill = def)) +
             geom_point(shape = 21, size = 4) +
             labs(x = "", y = "N interactions")
             #theme(legend.position ="none")
 
sp3<-sp3 + scale_fill_continuous(low = "#56B1F7", high = "#132B43")
sp3
# 2. Save the legend
#+++++++++++++++++++++++
legend <- get_legend(sp3) ### here is the tricky to get the legend in the right place in the multiple panel: get it from the graph, create an object, then remove if from the original graph
sp3 <- sp3 + theme(legend.position="none")

#eve<-data$evenness+0.5514/(1-data$evenness+0.5514)

#1. Scatter plots
sp4 <- ggplot(data, aes(x = logarea, y =links, size = 4, fill = def)) +
             geom_point(shape = 21) +
             labs(x = "", y = "N links per species")
sp4<-sp4 + scale_fill_continuous(low = "#56B1F7", high = "#132B43") +
            theme(legend.position="none")
sp4
## ≠≠≠≠≠≠≠≠≠≠≠≠≠≠≠≠≠≠≠≠≠≠≠≠
h2<-log(data$h2+0.0673)/(1-data$h2+0.0673)
sp5 <- ggplot(data, aes(x = logarea, y = h2, size = 4, fill = def)) +
             geom_point(shape = 21) +
             labs(x = "Fragment area (log[ha])", y = "H2'specialization")
sp5<-sp5 + scale_fill_continuous(low = "#56B1F7", high = "#132B43") +
          theme(legend.position ="none")
sp5
## ≠≠≠≠≠≠≠≠≠≠≠≠≠≠≠≠≠≠≠≠≠≠≠≠
sp6 <- ggplot(data, aes(x = logarea, y = wnodf, size = 4, fill = def)) +
             geom_point(shape = 21) +
             labs(x = "", y = "Weighted nestedness") +
   theme(legend.position ="none")
 
sp6<-sp6 + scale_fill_continuous(low = "#56B1F7", high = "#132B43")
sp6
#4 ≠≠≠≠≠≠≠≠≠≠≠≠≠≠≠≠≠≠≠≠≠≠≠≠
grid.arrange(sp1, sp2, sp3, legend,
             sp4, sp5, sp6, ncol=4, nrow=2,
             widths = c(6,6,6,2), heights = c(2,2))
```


```{r plot bipartiteD3, include = FALSE}

test1<-melt(as.matrix(Intervales))
test1 <- subset(test1, value>0)
test1$webID <- c(rep("Intervales"))

colnames(test1)[1]<-"lower"
colnames(test1)[2]<-"higher"
colnames(test1)[3]<-"freq"

bipartite::frame2webs(test1)-> webtest

bipartite_D3(webtest, PrimaryLab = 'Plants',
            SecondaryLab = 'Birds',
            colouroption = 'brewer',
            Orientation = 'horizontal', ColourBy = 1)


```


```{r networks igraph, include = FALSE}
#################
##igraph
m1<-melt(as.matrix(fragmentRJ)) 
m2 <- subset(m1, value>0)

g=graph.data.frame(m2,directed=FALSE) 
E(g)$weight=as.numeric(m2[,3]) #We then add the edge weights to this network by assigning an edge attribute called 'weight'.
#V(g)$name
#V(g)$color[1:184] = "blue"
#V(g)$color[185:265] = "orange"

V(g)$color <- ifelse(V(g)$name == str_match(V(g)$name, " "), "orange", "blue") #plants
V(g)$color <- ifelse(is.na(V(g)$color), "orange", "blue")

plot(g,
     layout=layout.circle,
         vertex.size=6,
     #main = '15',
         vertex.color = V(g)$color,
               edge.color = "gray80",
               edge.curved=0.3,
     vertex.label.dist=200,
     vertex.label.color='black',
     vertex.label.font=1,
     vertex.label=V(g)$name,
     vertex.label.cex=0.5,
     edge.width=E(g)$weight/4
)
```

```{r save RData}
save.image("network_area.RData")
```

